{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0,'..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flight_tables.heathrow_parsing import extract_batch_heathrow\n",
    "from flight_tables.flight_parsing import ParsedFlights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = '2020*Z.json' #'2020-01-29Z.json'\n",
    "root_dir = os.path.dirname(os.getcwd()) # parent of cwd\n",
    "data_dir = os.path.join(root_dir, 'data\\\\heathrow_data\\\\')\n",
    "path_pattern = os.path.join(data_dir, file_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files to Load\n",
    "files = glob.glob(path_pattern) # A List of file paths\n",
    "print(f\"{len(files)} files found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_df(path):\n",
    "    \"\"\"Load a Heathrow Flights JSON into a Dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "        path (str): path of JSON file you want to load.\n",
    "    Returns: \n",
    "        heathrow_df (pd.DataFrame): Dataframe created after parsing the raw file.\n",
    "    \"\"\"    \n",
    "    with open(path, 'r') as f:\n",
    "        heathrow_raw_dict = json.load(f)\n",
    "\n",
    "    batch_info = extract_batch_heathrow(heathrow_raw_dict)\n",
    "\n",
    "    parsed_flights = ParsedFlights(batch_info)\n",
    "\n",
    "    heathrow_df = parsed_flights.to_dataframe()\n",
    "    \n",
    "    return heathrow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_load_heathrow_json(file_paths):\n",
    "    \"\"\"Load a list of heathrow JSON files into a single DataFrame.\n",
    "    \n",
    "    Parameters (list): File path strings of the JSON files you want to load\n",
    "    Returns (pd.DataFrame): Single DataFrame with all the files you loaded.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    flight_dataframes = []\n",
    "\n",
    "    for file in files:\n",
    "        temp_df = file_to_df(file)\n",
    "        flight_dataframes.append(temp_df)\n",
    "\n",
    "    df = pd.concat(flight_dataframes)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes some time if loading many files... (consider adding progress bar)\n",
    "df = batch_load_heathrow_json(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickling\n",
    "After the first time you can save jsons as dataframe pickle and load from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.to_pickle(df, './all_flights.pkl') # Saves to cwd (Notebooks directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('all_flights.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe Preparation\n",
    "* Drop Duplicates\n",
    "* Drop Flights which are not the primary Flight ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Duplicates\n",
    "df.drop_duplicates(inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicate Analysis:\n",
    "#df.loc[df.duplicated()] # Show duplicates\n",
    "#df.loc[(df.delay_mins==16) & (df.flight_id=='BR068')] # Find specific Duplicates\n",
    "#assert df.duplicated().any()==False, \"Duplicated Entries found in Table.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Alternative flight IDs\n",
    "df = df.loc[df.code_share != 'alt_code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of dates matches number of files\n",
    "dates_count = len(df.scheduled_datetime.dt.date.unique())\n",
    "assert dates_count == len(files), \\\n",
    "    f\"Number of files doesn't match number of dates. \\n\\t You have {len(files)} files but data for {dates_count} dates.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flight_env",
   "language": "python",
   "name": "flight_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
